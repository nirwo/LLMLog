# LLM Configuration
# URL for your local LLM API
LLM_API_URL=http://localhost:11434/api/generate

# Model name to use
# For Ollama, options include: llama3, codellama, mistral
LLM_MODEL=codellama

# Timeout for LLM requests in seconds
LLM_TIMEOUT=30
