# LLM Configuration
# URL for your local LLM API
LLM_API_URL=http://localhost:11434/api/chat

# Model name to use
# For Ollama, options include: llama3, codellama, mistral
LLM_MODEL=llama3

# Timeout for LLM requests in seconds
LLM_TIMEOUT=30

# Fallback LLM Configuration (OpenAI API)
# Set to "true" to enable fallback to OpenAI API when local LLM is unavailable
USE_FALLBACK_LLM=true

# Your OpenAI API key
OPENAI_API_KEY=your_openai_api_key_here

# OpenAI model to use for fallback
# Options include: gpt-3.5-turbo, gpt-4, etc.
OPENAI_MODEL=gpt-3.5-turbo
